{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Majorproject.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoexbMiOsGgv"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.ticker as ticker\n",
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "\n",
        "wm_title=[]  \n",
        "wm_date = []\n",
        "wm_content = []\n",
        "wm_rating = []\n",
        "for i in range(1,150):\n",
        "    link =\"https://www.amazon.in/New-Apple-iPhone-Pro-512GB/product-reviews/B08L5WVZJ7/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews\"+str(i)\n",
        "    response = requests.get(link)\n",
        "    soup = bs(response.content,\"html.parser\")\n",
        "    \n",
        "    \n",
        "    \n",
        "# extracting Review Title\n",
        "  title = soup.find_all('a',class_='review-title-content')\n",
        "  review_title = []\n",
        "  for i in range(0,len(title)):\n",
        "      review_title.append(title[i].get_text())\n",
        "  review_title[:] = [titles.lstrip('\\n') for titles in review_title]\n",
        "  review_title[:] = [titles.rstrip('\\n') for titles in review_title]\n",
        "  wm_title = wm_title + review_title\n",
        "  \n",
        "  \n",
        "## Extracting Ratings\n",
        "  rating = soup.find_all('i',class_='review-rating')\n",
        "  review_rating = []\n",
        "  for i in range(2,len(rating)):\n",
        "      review_rating.append(rating[i].get_text())\n",
        "  #review_rating.pop(0)\n",
        "  #review_rating.pop(0)\n",
        "  review_rating[:] = [reviews.rstrip(' out of 5 stars') for reviews in review_rating]\n",
        "  wm_rating = wm_rating + review_rating  \n",
        "  \n",
        "  \n",
        "  \n",
        "#Extracting Content of review\n",
        "  review = soup.find_all(\"span\",{\"data-hook\":\"review-body\"})\n",
        "  review_content = []\n",
        "  for i in range(0,len(review)):\n",
        "      review_content.append(review[i].get_text())\n",
        "  review_content[:] = [reviews.lstrip('\\n') for reviews in review_content]\n",
        "  review_content[:] = [reviews.rstrip('\\n') for reviews in review_content]\n",
        "  wm_content = wm_content + review_content  \n",
        "\n",
        "\n",
        "#Extracting dates of reviews\n",
        "  dates = soup.find_all('span',class_='review-date')\n",
        "  review_dates = []\n",
        "  for i in range(2,len(rating)):\n",
        "      review_dates.append(dates[i].get_text())\n",
        "  review_dates[:] = [reviews.lstrip('Reviewed in India on') for reviews in review_dates]\n",
        "  #review_dates.pop(0)\n",
        "  #review_dates.pop(0)\n",
        "  wm_date  = wm_date + review_dates\n",
        "  \n",
        "  \n",
        "print(len(wm_title))\n",
        "print(len(wm_rating))\n",
        "print(len(wm_content))\n",
        "print(len(wm_date))\n",
        "\n",
        "\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df['Title'] = wm_title\n",
        "df['Ratings'] = wm_rating\n",
        "df['Comments'] = wm_content\n",
        "df['Date'] = wm_date\n",
        "\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Ratings'] = df['Ratings'].astype(float)\n",
        "df.head(2)\n",
        "\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def get_wordnet_pos(pos_tag):\n",
        "    if pos_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif pos_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif pos_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif pos_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "    \n",
        "import string\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    # lower text\n",
        "    text = text.lower()\n",
        "    # tokenize text and remove puncutation\n",
        "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
        "    # remove words that contain numbers\n",
        "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
        "    # remove stop words\n",
        "    stop = stopwords.words('english')\n",
        "    text = [x for x in text if x not in stop]\n",
        "    # remove empty tokens\n",
        "    text = [t for t in text if len(t) > 0]\n",
        "    # pos tag text\n",
        "    pos_tags = pos_tag(text)\n",
        "    # lemmatize text\n",
        "    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
        "    # remove words with only one letter\n",
        "    text = [t for t in text if len(t) > 1]\n",
        "    # join all\n",
        "    text = \" \".join(text)\n",
        "    return(text)\n",
        "\n",
        "\n",
        "# clean text data\n",
        "df[\"Comments\"] = df[\"Comments\"].apply(lambda x: clean_text(x))\n",
        "df['Title'] = df['Title'].astype(str)\n",
        "df['Title'] = df['Title'].apply(lambda x: clean_text(x))\n",
        "df.head(5)\n",
        "#  add sentiment anaylsis columns\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "df[\"sentiments\"] = df[\"Comments\"].apply(lambda x: sid.polarity_scores(x))\n",
        "df = pd.concat([df.drop(['sentiments'], axis=1), df['sentiments'].apply(pd.Series)], axis=1)\n",
        "\n",
        "df\n",
        "\n",
        "# add number of characters column\n",
        "df[\"nb_chars\"] = df[\"Comments\"].apply(lambda x: len(x))\n",
        "\n",
        "# add number of words column\n",
        "df[\"nb_words\"] = df[\"Comments\"].apply(lambda x: len(x.split(\" \")))\n",
        "\n",
        "# create doc2vec vector columns\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(df[\"Comments\"].apply(lambda x: x.split(\" \")))]\n",
        "\n",
        "# train a Doc2Vec model with our text data\n",
        "model = Doc2Vec(documents, vector_size=5, window=2, min_count=1, workers=4)\n",
        "\n",
        "# transform each Comment into a vector data\n",
        "doc2vec_df = df[\"Comments\"].apply(lambda x: model.infer_vector(x.split(\" \"))).apply(pd.Series)\n",
        "doc2vec_df.columns = [\"doc2vec_vector_\" + str(x) for x in doc2vec_df.columns]\n",
        "df = pd.concat([df, doc2vec_df], axis=1)\n",
        "\n",
        "# add tf-idfs columns\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(min_df = 6)\n",
        "tfidf_result = tfidf.fit_transform(df[\"Comments\"]).toarray()\n",
        "tfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names())\n",
        "tfidf_df.columns = [\"word_\" + str(x) for x in tfidf_df.columns]\n",
        "tfidf_df.index = df.index\n",
        "df = pd.concat([df, tfidf_df], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_wordcloud(data, title = None):\n",
        "    wordcloud = WordCloud(\n",
        "        background_color = 'white',\n",
        "        max_words = 200,\n",
        "        max_font_size = 40, \n",
        "        scale = 3,\n",
        "        random_state = 42\n",
        "    ).generate(str(data))\n",
        "\n",
        "    fig = plt.figure(1, figsize = (20, 20))\n",
        "    plt.axis('off')\n",
        "    if title: \n",
        "        fig.suptitle(title, fontsize = 20)\n",
        "        fig.subplots_adjust(top = 2.3)\n",
        "\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.show()\n",
        "show_wordcloud(df[\"Comments\"])\n",
        "show_wordcloud(df[\"Title\"])\n",
        "\n",
        "\n",
        "# highest positive sentiment reviews (with more than 5 words)\n",
        "df[df[\"nb_words\"] >= 5].sort_values(\"pos\", ascending = False)[[\"Comments\", \"pos\"]].head(10)\n",
        "# lowest negative sentiment reviews (with more than 5 words)\n",
        "df[df[\"nb_words\"] >= 5].sort_values(\"neg\", ascending = False)[[\"Comments\", \"neg\"]].head(10)\n",
        "\n",
        "\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df.head(2)\n",
        "\n",
        "df_recent = df[(df['Year']== 2020) & (df['Month'] != 8)]\n",
        "df_recent.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}